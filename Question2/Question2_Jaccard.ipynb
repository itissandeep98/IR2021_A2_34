{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripSpecialChar(text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "def preProcess(text):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        # stemmedWords = list([stemmer.stem(word) for word in text_tokens])\n",
    "        # validTokens = [i for i in stemmedWords if i not in stopWords]\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return set(validTokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findJaccardCoefficient(query, filePaths):\n",
    "    # query is coming preprocessed\n",
    "    for i,filePath in enumerate(tqdm(filePaths)):\n",
    "        try:\n",
    "            file = open(filePath, encoding=\"utf8\")\n",
    "            read = file.read().replace('\\n', ' ')    \n",
    "        except Exception as e:\n",
    "            file = open(filePath, encoding=\"unicode_escape\")\n",
    "            read = file.read().replace('\\n', ' ')\n",
    "        file.close()\n",
    "        # this step can be done faster if we\n",
    "        # store all the prePreprocssed documents in a file\n",
    "        \n",
    "        \n",
    "        processedDoc = preProcess(read)                    \n",
    "        # Finding Intersection and Union of Doc,Query\n",
    "        DocInterQ  = set.intersection(processedDoc, query)\n",
    "        DocUnionQ  = set.union(processedDoc, query)\n",
    "        \n",
    "        coeff = len(DocInterQ)/len(DocUnionQ)    #Jaccard Coefficient value\n",
    "        db[filePath] = coeff                     #Dict. with key as filename and value as jaccard coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the query: lion cat hero\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20e6d9bb3ad42f88dc751abee000fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Jaccard Coefficient :    Document\n",
      "0.00819672131147541    :stories/lionwar.txt\n",
      "0.007751937984496124    :stories/lionmosq.txt\n",
      "0.007462686567164179    :stories/mouslion.txt\n",
      "0.006802721088435374    :stories/pussboot.txt\n",
      "0.006493506493506494    :stories/burltrs\n"
     ]
    }
   ],
   "source": [
    "allFiles = os.walk(\"../Dataset/stories\")\n",
    "filePaths = []\n",
    "for i in allFiles:\n",
    "    for j in i[2]:\n",
    "        filePath = i[0] + \"/\" + j\n",
    "        filePaths.append(filePath)\n",
    "\n",
    "db = {}        \n",
    "sentence_query = input(\"Enter the query: \")\n",
    "query = preProcess(sentence_query)      # Query Processing\n",
    "findJaccardCoefficient(query, filePaths)\n",
    "\n",
    "# Printing the top 5 documents\n",
    "k = Counter(db)\n",
    "topFiveDoc = k.most_common(5) \n",
    "print(\" Jaccard Coefficient :    Document\")  \n",
    "for i in topFiveDoc:\n",
    "    print(\"{}    :{}\".format(i[1],i[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}