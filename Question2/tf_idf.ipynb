{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "class InvertedIndex():\n",
    "    def __init__(self):\n",
    "        self.DFpostings={}\n",
    "        self.tokens_files=[]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Keys:\"+str(len(list(self.db.keys())))\n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        # stemmedWords = list([stemmer.stem(word) for word in text_tokens])\n",
    "        # validTokens = [i for i in stemmedWords if i not in stopWords]\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens, set(validTokens)\n",
    "\n",
    "    def indexFile(self,file,fileId):\n",
    "        '''\n",
    "        Indexes the file pass as argument to the associated fileID\n",
    "        '''\n",
    "        tokens, setTokens = self.preProcess(file)\n",
    "        self.tokens_files.append(tokens)\n",
    "        for i in setTokens:\n",
    "            if i in self.DFpostings:\n",
    "                self.DFpostings[i].append(fileId)\n",
    "            else:\n",
    "                self.DFpostings[i] = [fileId]\n",
    "\n",
    "    def generateWordcloud(self):\n",
    "        '''\n",
    "        Creates a wordclound to visualize the frequence of words in the index\n",
    "        '''\n",
    "        frequencyDict = {}\n",
    "        for key in self.db:\n",
    "            frequencyDict[key] = len(self.db[key])\n",
    "        wordcloud = WordCloud().generate_from_frequencies(frequencyDict)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    def get_fileTokens(i):\n",
    "        return self.tokens_files(i)\n",
    "\n",
    "    def save(self,file):\n",
    "        '''\n",
    "        Save the index to a file locally\n",
    "        '''\n",
    "        json.dump(self.DFpostings, open(file, \"w\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('PostingsAndTokens2.obj','rb') as file_object:\n",
    "    raw_data = file_object.read()\n",
    "work_obj = pickle.loads(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idfmatrices():\n",
    "    def __init__(self,DFpostings, tokens, docs_count):\n",
    "        self.DFpostings = DFpostings\n",
    "        self.tokens = tokens\n",
    "        self.docs_count = docs_count\n",
    "        self.vocab_count = len(DFpostings)\n",
    "        self.vocabulary = [x for x in DFpostings]\n",
    "        self.idf = dict()\n",
    "        self.counter_lists = []\n",
    "        self.most_common2 = []\n",
    "\n",
    "        self.tf_idf_Binary = np.zeros((docs_count, self.vocab_count))\n",
    "        self.tf_idf_RawCount = np.zeros((docs_count, self.vocab_count))\n",
    "        self.tf_idf_TermFreq = np.zeros((docs_count, self.vocab_count))\n",
    "        self.tf_idf_LogNorm = np.zeros((docs_count, self.vocab_count))\n",
    "        self.tf_idf_DoubleNorm = np.zeros((docs_count, self.vocab_count))\n",
    "    \n",
    "    def generateIDF(self):\n",
    "         for key in self.DFpostings:\n",
    "                c = len(self.DFpostings[key])\n",
    "                c=c+1                      #smoothihg factor\n",
    "                self.idf[key] = np.log10(self.docs_count/c)\n",
    "        \n",
    "    def generateCounterLists(self):\n",
    "        for i in tnrange(self.docs_count):\n",
    "            self.counter_lists.append(Counter(self.tokens[i]))\n",
    "            \n",
    "    def generateMostCommon2(self):\n",
    "        for i in tnrange(self.docs_count):\n",
    "            self.most_common2.append((self.counter_lists[i]).most_common(2))\n",
    "    \n",
    "    def generateBinary(self):\n",
    "        for i in tnrange(len(self.vocabulary)):\n",
    "            for j in range(self.docs_count):\n",
    "                if(self.vocabulary[i] in self.tokens[j]):\n",
    "                    self.tf_idf_Binary[j][i]=1*self.idf[self.vocabulary[i]]\n",
    "                    \n",
    "    def generateRawCount(self):\n",
    "        for i in tnrange(len(self.vocabulary)):\n",
    "            for j in range(self.docs_count):\n",
    "                self.tf_idf_RawCount[j][i] = self.counter_lists[j][self.vocabulary[i]]*self.idf[self.vocabulary[i]]\n",
    "    \n",
    "    def generateTermFreq(self):\n",
    "        for i in tnrange(len(self.vocabulary)):\n",
    "            for j in range(self.docs_count):\n",
    "                self.tf_idf_TermFreq[j][i] = (self.counter_lists[j][self.vocabulary[i]]/len(self.tokens[j]))*self.idf[self.vocabulary[i]]\n",
    "    \n",
    "    def generateLogNorm(self):\n",
    "        for i in tnrange(len(self.vocabulary)):\n",
    "            for j in range(self.docs_count):\n",
    "                self.tf_idf_LogNorm[j][i] = np.log10(1+self.counter_lists[j][self.vocabulary[i]])*self.idf[self.vocabulary[i]]\n",
    "    \n",
    "    def generateDoubleNorm(self):\n",
    "        for i in tnrange(len(self.vocabulary)):\n",
    "            for j in range(self.docs_count):\n",
    "                maxnum=0;\n",
    "                if(self.most_common2[j][0][0]==self.vocabulary[i]):\n",
    "                    maxnum = self.most_common2[j][1][1]\n",
    "                else:\n",
    "                    maxnum = self.most_common2[j][0][1]            \n",
    "                self.tf_idf_DoubleNorm[j][i] = (0.5+0.5*(self.counter_lists[j][self.vocabulary[i]]/maxnum))*self.idf[self.vocabulary[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7309342e6166477581ac9aa3b0361555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275bb817d3f7418581321e53cf656d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/467 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56262f7adfbd4718a40ac8f95cf588a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba4cba5a46c4b2c91694c976069918c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dfc78f4cc440e693bc5d4659871a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc831245cf6045eb86cfce4ecc3accc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3b8ad2b91c4f1e80d7c2aa3cd22a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54867 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "work_objthis = tf_idfmatrices(work_obj.DFpostings, work_obj.tokens_files,467)\n",
    "work_objthis.generateIDF()\n",
    "work_objthis.generateCounterLists()\n",
    "work_objthis.generateMostCommon2()\n",
    "work_objthis.generateBinary()\n",
    "work_objthis.generateRawCount()\n",
    "work_objthis.generateTermFreq()\n",
    "work_objthis.generateLogNorm()\n",
    "work_objthis.generateDoubleNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# serialized = pickle.dumps(work_objthis)\n",
    "# filename = 'TF_IDF_Calculated.obj'\n",
    "# with open(filename,'wb') as file_object:\n",
    "#     file_object.write(serialized)\n",
    "\n",
    "\n",
    "with open('TF_IDF_Calculated.obj','rb') as file_object:\n",
    "    raw_data = file_object.read()\n",
    "work_objthis = pickle.loads(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "def stripSpecialChar(text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "    \n",
    "def preProcess(text):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        # stemmedWords = list([stemmer.stem(word) for word in text_tokens])\n",
    "        # validTokens = [i for i in stemmedWords if i not in stopWords]\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class queryProcessing():\n",
    "    def __init__(self, queryList, work_objthis):\n",
    "        self.queryList = queryList\n",
    "        self.work_objthis = work_objthis\n",
    "        self.query_eval = np.zeros((work_objthis.docs_count,5))\n",
    "    def compute_Scores(self):\n",
    "        indextolookfor = []\n",
    "        for word in self.queryList:\n",
    "            index = self.work_objthis.vocabulary.index(word)\n",
    "            indextolookfor.append(index)\n",
    "        for docs in range(self.work_objthis.docs_count):\n",
    "            for query in indextolookfor:\n",
    "                self.query_eval[docs][0]+= self.work_objthis.tf_idf_Binary[docs][query]\n",
    "                self.query_eval[docs][1]+= self.work_objthis.tf_idf_RawCount[docs][query]\n",
    "                self.query_eval[docs][2]+= self.work_objthis.tf_idf_TermFreq[docs][query]\n",
    "                self.query_eval[docs][3]+= self.work_objthis.tf_idf_LogNorm[docs][query]\n",
    "                self.query_eval[docs][4]+= self.work_objthis.tf_idf_DoubleNorm[docs][query]\n",
    "                \n",
    "    def Top5(self,alist):\n",
    "        return sorted(range(len(alist)), key=lambda i: alist[i], reverse=True)[:5]\n",
    "\n",
    "    \n",
    "    def results(self):\n",
    "        print(\"For Binary TF-IDF, Top 5 documents:\")\n",
    "        print(self.Top5(self.query_eval[:,0]))\n",
    "        print(\"\")\n",
    "        print(\"For RawCount TF-IDF, Top 5 documents:\")\n",
    "        print(self.Top5(self.query_eval[:,1]))\n",
    "        print(\"\")\n",
    "        print(\"For TermFreq TF-IDF, Top 5 documents:\")\n",
    "        print(self.Top5(self.query_eval[:,2]))\n",
    "        print(\"\")\n",
    "        print(\"For LogNorm TF-IDF, Top 5 documents:\")\n",
    "        print(self.Top5(self.query_eval[:,3]))\n",
    "        print(\"\")\n",
    "        print(\"For DoubleNorm TF-IDF, Top 5 documents:\")\n",
    "        print(self.Top5(self.query_eval[:,4]))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sentence:-king kong\n",
      "For Binary TF-IDF, Top 5 documents:\n",
      "[276, 402, 6, 8, 15]\n",
      "\n",
      "For RawCount TF-IDF, Top 5 documents:\n",
      "[214, 230, 26, 160, 169]\n",
      "\n",
      "For TermFreq TF-IDF, Top 5 documents:\n",
      "[286, 335, 271, 230, 160]\n",
      "\n",
      "For LogNorm TF-IDF, Top 5 documents:\n",
      "[214, 230, 26, 160, 169]\n",
      "\n",
      "For DoubleNorm TF-IDF, Top 5 documents:\n",
      "[160, 230, 286, 15, 399]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_query = input(\"Enter the sentence:-\")\n",
    "listofwords = preProcess(sentence_query)\n",
    "evaluation = queryProcessing(listofwords,work_objthis)\n",
    "evaluation.compute_Scores()\n",
    "evaluation.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine - Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bf0ef78c2c46318ee53cd1a05aa8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb8a493a6584dda9fc4608e101ff390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c433f3bb8f422c9b9f73d7ca0b5210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=54867), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebb599a8cae4955975d3046e7c88de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=54867), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb80106c22a4e6aa30f006f89148de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=54867), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92b0469b7ac43ebb86e4701738881a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=54867), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b08e22e15004e52a34d7c33a035081b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=54867), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "query_obj = tf_idfmatrices(work_obj.DFpostings, [listofwords], 1)\n",
    "query_obj.idf = work_objthis.idf\n",
    "\n",
    "### Do Not Uncomment!!! ####\n",
    "# query_obj.generateIDF()\n",
    "\n",
    "query_obj.generateCounterLists()\n",
    "query_obj.generateMostCommon2()\n",
    "query_obj.generateBinary()\n",
    "query_obj.generateRawCount()\n",
    "query_obj.generateTermFreq()\n",
    "query_obj.generateLogNorm()\n",
    "query_obj.generateDoubleNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(query_obj.tf_idf_Binary.shape)\n",
    "# print(query_obj.tf_idf_RawCount.shape)\n",
    "# print(query_obj.tf_idf_TermFreq.shape)\n",
    "# print(query_obj.tf_idf_LogNorm.shape)\n",
    "# print(query_obj.tf_idf_DoubleNorm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine_Similarity(Q,D,scheme):\n",
    "    \"\"\"\n",
    "    Q -> query vector (1 x vocab_count)\n",
    "    D -> document vector (n_doc x vocab_count)\n",
    "    \"\"\"\n",
    "    n_doc = D.shape[0]\n",
    "    Z = np.zeros((n_doc, 1))\n",
    "    \n",
    "    Q_ = Q.T\n",
    "    for i in range(n_doc):\n",
    "        Z[i,0] = dot(D[i,:],Q_[:,0]) / ( norm(D[i,:]) * norm(Q_[:,0]))\n",
    "    \n",
    "    doc_str = [str(i) for i in range(n_doc)]\n",
    "    \n",
    "    sim = list(zip(Z[:,0],doc_str))\n",
    "    sim.sort(reverse=True)\n",
    "    files = [int(x[1]) for x in sim[:5]]\n",
    "    scores = [float(x[0]) for x in sim[:5]]\n",
    "    \n",
    "    print(\"------ Scheme :: \" + scheme + \" ------\")\n",
    "    print(\"------ Top 5 Matching Documents ------\")    \n",
    "    \n",
    "    mapping = json.load(open('mapping.json'))\n",
    "    \n",
    "    data = [[i,mapping[i]] for i in files]\n",
    "    for i in range(len(data)):\n",
    "        data[i].append(scores[i])\n",
    "\n",
    "    print(tabulate(data,headers=['Document ID','Location','Scores']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity - Binary scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scheme :: Binary ------\n",
      "------ Top 5 Matching Documents ------\n",
      "  Document ID  Location                 Scores\n",
      "-------------  --------------------  ---------\n",
      "          276  stories/mario.txt     0.0833187\n",
      "          402  stories/superg1       0.0330532\n",
      "          267  stories/lionwar.txt   0.0234361\n",
      "          266  stories/lionmosq.txt  0.0220713\n",
      "          186  stories/foxncrow.txt  0.0218335\n"
     ]
    }
   ],
   "source": [
    "Cosine_Similarity(query_obj.tf_idf_Binary,work_objthis.tf_idf_Binary,scheme=\"Binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity - Raw Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scheme :: Raw Count ------\n",
      "------ Top 5 Matching Documents ------\n",
      "  Document ID  Location                 Scores\n",
      "-------------  --------------------  ---------\n",
      "          160  stories/fable.txt     0.117795\n",
      "          230  stories/hop-frog.poe  0.0962945\n",
      "          286  stories/monkking.txt  0.090235\n",
      "           15  stories/6ablemen.txt  0.0801088\n",
      "          335  stories/pussboot.txt  0.0708097\n"
     ]
    }
   ],
   "source": [
    "Cosine_Similarity(query_obj.tf_idf_RawCount,work_objthis.tf_idf_RawCount,scheme=\"Raw Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity - Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scheme :: Term Frequency ------\n",
      "------ Top 5 Matching Documents ------\n",
      "  Document ID  Location                 Scores\n",
      "-------------  --------------------  ---------\n",
      "          160  stories/fable.txt     0.117795\n",
      "          230  stories/hop-frog.poe  0.0962945\n",
      "          286  stories/monkking.txt  0.090235\n",
      "           15  stories/6ablemen.txt  0.0801088\n",
      "          335  stories/pussboot.txt  0.0708097\n"
     ]
    }
   ],
   "source": [
    "Cosine_Similarity(query_obj.tf_idf_TermFreq,work_objthis.tf_idf_TermFreq,scheme=\"Term Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity - Log Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scheme :: Log Normalization ------\n",
      "------ Top 5 Matching Documents ------\n",
      "  Document ID  Location                 Scores\n",
      "-------------  --------------------  ---------\n",
      "          276  stories/mario.txt     0.0990284\n",
      "          286  stories/monkking.txt  0.0549539\n",
      "          335  stories/pussboot.txt  0.041418\n",
      "           15  stories/6ablemen.txt  0.0399768\n",
      "          271  stories/lpeargrl.txt  0.0395454\n"
     ]
    }
   ],
   "source": [
    "Cosine_Similarity(query_obj.tf_idf_LogNorm,work_objthis.tf_idf_LogNorm,scheme=\"Log Normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity - Double Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Scheme :: Double Normalization ------\n",
      "------ Top 5 Matching Documents ------\n",
      "  Document ID  Location               Scores\n",
      "-------------  -------------------  --------\n",
      "          167  stories/fear.hum     0.999988\n",
      "          287  stories/monksol.txt  0.999988\n",
      "           53  stories/bestwish     0.999987\n",
      "           96  stories/bumm         0.999987\n",
      "          293  stories/myeyes       0.999987\n"
     ]
    }
   ],
   "source": [
    "Cosine_Similarity(query_obj.tf_idf_DoubleNorm,work_objthis.tf_idf_DoubleNorm,scheme=\"Double Normalization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
