{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd00677a19ce35e1848297257c6ee5dd2327c2a9e8095762e1bb79199eb1274a5ac",
   "display_name": "Python 3.8.5 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "0677a19ce35e1848297257c6ee5dd2327c2a9e8095762e1bb79199eb1274a5ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandeep/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/sandeep/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "from functools import reduce\n",
    "import copy\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "source": [
    "# Positional Index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalIndex():\n",
    "    def __init__(self):\n",
    "        self.db={}\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Keys:\"+str(len(list(self.db.keys())))\n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens\n",
    "\n",
    "    def indexFile(self,file,fileId):\n",
    "        '''\n",
    "        Indexes the file pass as argument to the associated fileID\n",
    "        '''\n",
    "        tokens = self.preProcess(file)\n",
    "        for pos,val in enumerate(tokens):\n",
    "            if val in self.db:\n",
    "                self.db[val][0]+=1\n",
    "                if(fileId in self.db[val][1]):\n",
    "                    self.db[val][1][fileId].append(pos)\n",
    "                else:\n",
    "                    self.db[val][1][fileId]=[pos]\n",
    "                \n",
    "            else:\n",
    "                self.db[val] = [1,{fileId:[pos]}]\n",
    "\n",
    "    def generateWordcloud(self):\n",
    "        '''\n",
    "        Creates a wordclound to visualize the frequence of words in the index\n",
    "        '''\n",
    "        frequencyDict = {}\n",
    "        for key in self.db:\n",
    "            frequencyDict[key] = len(self.db[key])\n",
    "        wordcloud = WordCloud().generate_from_frequencies(frequencyDict)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    def save(self):\n",
    "        '''\n",
    "        Save the index to a file locally\n",
    "        '''\n",
    "        json.dump(self.db, open('output.json', \"w\"))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/467 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7abd150118a04d91b61ffe6e587b31fe"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "allFiles = os.walk(\"Dataset/stories\")\n",
    "filePaths = []\n",
    "for i in allFiles:\n",
    "    for j in i[2]:\n",
    "        filePath = i[0] + \"/\" + j\n",
    "        filePaths.append(filePath)\n",
    "\n",
    "json.dump(filePaths, open(\"mapping.json\", \"w\"))\n",
    "\n",
    "index = PositionalIndex()\n",
    "\n",
    "for i,filePath in enumerate(tqdm(filePaths)):\n",
    "    try:\n",
    "        file = open(filePath, encoding=\"utf8\")\n",
    "        read = file.read().replace('\\n', ' ')    \n",
    "    except Exception as e:\n",
    "        file = open(filePath, encoding=\"unicode_escape\")\n",
    "        read = file.read().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    index.indexFile(read, i)\n",
    "    index.save()\n"
   ]
  },
  {
   "source": [
    "## Query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        initializes the object with loading the index file\n",
    "        '''\n",
    "        self.db=json.load(open('output.json'))\n",
    "        self.db=defaultdict(lambda:[],self.db)   \n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens\n",
    "\n",
    "    def intersectLists(self,lists):\n",
    "        if len(lists)==0:\n",
    "            return []\n",
    "       \n",
    "        lists.sort(key=len)\n",
    "        return list(reduce(lambda x,y: set(x)&set(y),lists))\n",
    "        \n",
    "    def getPostings(self, terms):\n",
    "        return [ self.db[term][1] for term in terms ]\n",
    "    \n",
    "    def getDocsFromPostings(self, postings):\n",
    "        return [ [x for x in p] for p in postings ]\n",
    "\n",
    "    def performQuery(self,phrase):\n",
    "        phrase=self.preProcess(phrase)\n",
    "    \n",
    "        if(len(self.db.keys())==0):\n",
    "            self.load()\n",
    "\n",
    "        for term in phrase:\n",
    "            if(term not in self.db):\n",
    "                return []\n",
    "\n",
    "        postings=self.getPostings(phrase)    #all the terms in q are in the index\n",
    "        docs=self.getDocsFromPostings(postings)\n",
    "        docs=self.intersectLists(docs)\n",
    "        # print(postings)\n",
    "        for i in range(len(postings)):\n",
    "            postings[i]=[postings[i][x] for x in postings[i] if x in docs]\n",
    "        \n",
    "        postings=copy.deepcopy(postings)    #this is important since we are going to modify the postings list\n",
    "        \n",
    "        for i in range(len(postings)):\n",
    "            for j in range(len(postings[i])):\n",
    "                postings[i][j]=[x-i for x in postings[i][j]]\n",
    "        return postings\n",
    "        \n",
    "        # #intersect the locations\n",
    "        # result=[]\n",
    "        # for i in xrange(len(postings[0])):\n",
    "        #     li=self.intersectLists( [x[i][1] for x in postings] )\n",
    "        #     if li==[]:\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         result.append(postings[0][i][0])    #append the docid to the result\n",
    "        \n",
    "        # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['aytori', 'psychia']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[[395,\n",
       "   421,\n",
       "   1200,\n",
       "   1396,\n",
       "   1402,\n",
       "   1443,\n",
       "   1684,\n",
       "   1698,\n",
       "   1716,\n",
       "   1760,\n",
       "   1851,\n",
       "   1865,\n",
       "   1871,\n",
       "   2102,\n",
       "   2194],\n",
       "  [854,\n",
       "   871,\n",
       "   902,\n",
       "   926,\n",
       "   1078,\n",
       "   1279,\n",
       "   1305,\n",
       "   1409,\n",
       "   1592,\n",
       "   2112,\n",
       "   2226,\n",
       "   2275,\n",
       "   2670,\n",
       "   2705,\n",
       "   2714,\n",
       "   2832,\n",
       "   2983,\n",
       "   3026,\n",
       "   3035,\n",
       "   3052]],\n",
       " [[384, 429, 1817, 2199], [89, 131, 1517, 1533, 1544, 2130, 2558]]]"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "query = Query()\n",
    "# phrase=list(input().split())\n",
    "query.performQuery(\"aytori psychia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}