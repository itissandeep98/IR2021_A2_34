{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd00677a19ce35e1848297257c6ee5dd2327c2a9e8095762e1bb79199eb1274a5ac",
   "display_name": "Python 3.8.5 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "0677a19ce35e1848297257c6ee5dd2327c2a9e8095762e1bb79199eb1274a5ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm,tnrange\n",
    "import string\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "from functools import reduce\n",
    "import copy\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "source": [
    "# Positional Index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalIndex():\n",
    "    def __init__(self):\n",
    "        self.db={}\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Keys:\"+str(len(list(self.db.keys())))\n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        # validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens\n",
    "\n",
    "    def indexFile(self,file,fileId):\n",
    "        '''\n",
    "        Indexes the file pass as argument to the associated fileID\n",
    "        '''\n",
    "        tokens = self.preProcess(file)\n",
    "        for pos,val in enumerate(tokens):\n",
    "            if val in self.db:\n",
    "                self.db[val][0]+=1\n",
    "                if(fileId in self.db[val][1]):\n",
    "                    self.db[val][1][fileId].append(pos)\n",
    "                else:\n",
    "                    self.db[val][1][fileId]=[pos]\n",
    "                \n",
    "            else:\n",
    "                self.db[val] = [1,{fileId:[pos]}]\n",
    "\n",
    "    def generateWordcloud(self):\n",
    "        '''\n",
    "        Creates a wordclound to visualize the frequence of words in the index\n",
    "        '''\n",
    "        frequencyDict = {}\n",
    "        for key in self.db:\n",
    "            frequencyDict[key] = len(self.db[key])\n",
    "        wordcloud = WordCloud().generate_from_frequencies(frequencyDict)\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    def save(self):\n",
    "        '''\n",
    "        Save the index to a file locally\n",
    "        '''\n",
    "        json.dump(self.db, open('output.json', \"w\"))\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = os.walk(\"../Dataset/stories\")\n",
    "filePaths = []\n",
    "for i in allFiles:\n",
    "    for j in i[2]:\n",
    "        filePath = i[0] + \"/\" + j\n",
    "        filePaths.append(filePath)\n",
    "\n",
    "json.dump(filePaths, open(\"mapping.json\", \"w\"))\n",
    "\n",
    "index = PositionalIndex()\n",
    "\n",
    "for i,filePath in enumerate(tqdm(filePaths)):\n",
    "    try:\n",
    "        file = open(filePath, encoding=\"utf8\")\n",
    "        read = file.read().replace('\\n', ' ')    \n",
    "    except Exception as e:\n",
    "        file = open(filePath, encoding=\"unicode_escape\")\n",
    "        read = file.read().replace('\\n', ' ')\n",
    "    file.close()\n",
    "    index.indexFile(read, i)\n",
    "    index.save()\n"
   ]
  },
  {
   "source": [
    "## Query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query():\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        initializes the object with loading the index file\n",
    "        '''\n",
    "        self.db=json.load(open('output.json'))\n",
    "        self.db=defaultdict(lambda:[],self.db)   \n",
    "\n",
    "    def stripSpecialChar(self,text):\n",
    "        return ''.join(ch for ch in text if ch.isalnum() and not ch.isdigit() and ch not in string.punctuation)\n",
    "\n",
    "    def preProcess(self,text):\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "\n",
    "        text = text.lower()                                     # convert all text to lower case\n",
    "        text_tokens = word_tokenize(text)                       # tokenizing the text\n",
    "\n",
    "        validTokens = [i for i in text_tokens if i not in stopWords]    # removing stop words\n",
    "\n",
    "        validTokens = [self.stripSpecialChar(x) for x in validTokens]   # stripping special characters\n",
    "        validTokens = [x for x in validTokens if len(x) > 1]    # Choosing only words which has length > 1\n",
    "        return validTokens\n",
    "\n",
    "    def intersectLists(self,lists):\n",
    "        '''\n",
    "        Takes a 2D list and returns the intersection between them  \n",
    "        '''\n",
    "        if len(lists)==0:\n",
    "            return []\n",
    "       \n",
    "        lists.sort(key=len)\n",
    "        return list(reduce(lambda x,y: set(x)&set(y),lists))\n",
    "        \n",
    "    def getPostings(self, terms):\n",
    "        '''\n",
    "        Takes list of terms and returns a 3D list specifying docID with positions for every term\n",
    "        '''\n",
    "        return [ [ [ i, self.db[term][1][i] ] for i in self.db[term][1] ] for term in terms ]\n",
    "    \n",
    "    def getDocsFromPostings(self, postings):\n",
    "        '''\n",
    "        Takes list of postings and returns only the document id from that list for every term\n",
    "        '''\n",
    "        return [ [x[0] for x in p] for p in postings ]\n",
    "\n",
    "    def performQuery(self,phrase):\n",
    "        phrase=self.preProcess(phrase)    # Preprocessing of query\n",
    "        print(\"Query:\",phrase)\n",
    "        \n",
    "        if(len(self.db.keys())==0):      # if dataset is empty the loading it\n",
    "            self.load()\n",
    "        \n",
    "        for term in phrase:               # If any term does not exist in the dataset then return an empty list\n",
    "            if(term not in self.db):\n",
    "                return []\n",
    "\n",
    "        if(len(phrase)==1):                # If there is only 1 phrase left after preprocessing then result is only docs of that term in dataset\n",
    "            result= self.db[phrase[0]][1].keys()\n",
    "            result=list(result)\n",
    "\n",
    "        else:\n",
    "            postings=self.getPostings(phrase)    \n",
    "            docs=self.getDocsFromPostings(postings)\n",
    "            docs=self.intersectLists(docs)     # find common docs that contain all the terms\n",
    "\n",
    "            for i in range(len(postings)):      # Filtering Postings so that it contains only docs which contain all the terms\n",
    "                postings[i]=[x for x in postings[i] if x[0] in docs]\n",
    "            \n",
    "            postings=copy.deepcopy(postings)\n",
    "          \n",
    "            for i in range(len(postings)):      # Reducing positons of subsequent terms so that if all the terms are adjacent to each other then after this every posting list will have a common position\n",
    "                for j in range(len(postings[i])):\n",
    "                    postings[i][j][1]=[x-i for x in postings[i][j][1]]\n",
    "\n",
    "            \n",
    "            result=[]\n",
    "            for i in range(len(postings[0])):\n",
    "                intersection=self.intersectLists([x[i][1] for x in postings]) # finding intersections in postings\n",
    "                if len(intersection)>0:\n",
    "                    result.append(postings[0][i][0])            # if intersection found append the document id\n",
    "\n",
    "        result=list(map(int,result))    # convert list of strings to list of integers\n",
    "        self.getMapping(result)         # Showing fileID with thier locations\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "    def getMapping(self,files):\n",
    "        print('Total Documents:',len(files))\n",
    "\n",
    "        self.mapping=json.load(open('mapping.json'))\n",
    "        data= list(map(lambda i:(i,self.mapping[i]),files)) # list of tuples consisting of document id and their location\n",
    "        data.sort()\n",
    "        \n",
    "        print(tabulate(data,headers=['Document ID','Location']))\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Query()\n",
    "phrase=list(input().split())\n",
    "output=query.performQuery(phrase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}